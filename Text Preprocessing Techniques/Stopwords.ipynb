{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords"
      ],
      "metadata": {
        "id": "KJ1tBqjbK_7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words that are so widely used that they carry very little useful information.  Examples of stop words in English are “a,” “the,” “is,” “are,” etc."
      ],
      "metadata": {
        "id": "XXrqRTf9U4PZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "acbSWYeBK3Yc"
      },
      "outputs": [],
      "source": [
        "## Passage from French writer, novelist, diarist and essayist Anais Nin:\n",
        "\n",
        "paragraph=\"\"\"\"I will not accept a limited self. How can I affirmable self when I feel, in me, all possibilities?\n",
        "Allendy may have said: 'This is the core,' but I never feel the found substance of the self, the core.\n",
        "I feel only space, illimitable space. The effect of analysis is wearing off in this way, too.\n",
        "The clear, stark vibrations of the life I had soon after. I covered the truth again. The veil was sun.\n",
        "Eddies of illusion in mystery again engulfed reality. What I imagine is as true as what is.\n",
        "I want to get lost in mystery again. I have accepted a self which is wisdom engulfed by life.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "YlHO0lp6Ww3V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "LL_GLUZCXfjF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "laypD0JjVVVW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ehCAIvnfV5zV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1tt4rXRV9gN",
        "outputId": "0fdc3777-2852-4657-a645-1ff3de549c9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnRL9BYxXA2H",
        "outputId": "d00543c3-0335-4a58-d344-a8c2479c7ccd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0eCOVPHWMY2",
        "outputId": "d8365fbe-a390-4dab-a566-b16d615f7e57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('french')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3Ibm7R6WUGr",
        "outputId": "d665e781-126a-45e1-a40d-51ed3c1b1fdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['au',\n",
              " 'aux',\n",
              " 'avec',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'et',\n",
              " 'eux',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'je',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'même',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'ses',\n",
              " 'son',\n",
              " 'sur',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'c',\n",
              " 'd',\n",
              " 'j',\n",
              " 'l',\n",
              " 'à',\n",
              " 'm',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'y',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étants',\n",
              " 'étantes',\n",
              " 'suis',\n",
              " 'es',\n",
              " 'est',\n",
              " 'sommes',\n",
              " 'êtes',\n",
              " 'sont',\n",
              " 'serai',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'serons',\n",
              " 'serez',\n",
              " 'seront',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'serions',\n",
              " 'seriez',\n",
              " 'seraient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étions',\n",
              " 'étiez',\n",
              " 'étaient',\n",
              " 'fus',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fûtes',\n",
              " 'furent',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'soyons',\n",
              " 'soyez',\n",
              " 'soient',\n",
              " 'fusse',\n",
              " 'fusses',\n",
              " 'fût',\n",
              " 'fussions',\n",
              " 'fussiez',\n",
              " 'fussent',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eus',\n",
              " 'ai',\n",
              " 'as',\n",
              " 'avons',\n",
              " 'avez',\n",
              " 'ont',\n",
              " 'aurai',\n",
              " 'auras',\n",
              " 'aura',\n",
              " 'aurons',\n",
              " 'aurez',\n",
              " 'auront',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'aurions',\n",
              " 'auriez',\n",
              " 'auraient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avions',\n",
              " 'aviez',\n",
              " 'avaient',\n",
              " 'eut',\n",
              " 'eûmes',\n",
              " 'eûtes',\n",
              " 'eurent',\n",
              " 'aie',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'ayons',\n",
              " 'ayez',\n",
              " 'aient',\n",
              " 'eusse',\n",
              " 'eusses',\n",
              " 'eût',\n",
              " 'eussions',\n",
              " 'eussiez',\n",
              " 'eussent']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "metadata": {
        "id": "9kVZHzh-WbKT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "280YNl1bWmNX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f556TyQ_Wtc2",
        "outputId": "cc4d6a94-e599-4e39-d23f-3badf3251857"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"I will not accept a limited self.',\n",
              " 'How can I affirmable self when I feel, in me, all possibilities?',\n",
              " \"Allendy may have said: 'This is the core,' but I never feel the found substance of the self, the core.\",\n",
              " 'I feel only space, illimitable space.',\n",
              " 'The effect of analysis is wearing off in this way, too.',\n",
              " 'The clear, stark vibrations of the life I had soon after.',\n",
              " 'I covered the truth again.',\n",
              " 'The veil was sun.',\n",
              " 'Eddies of illusion in mystery again engulfed reality.',\n",
              " 'What I imagine is as true as what is.',\n",
              " 'I want to get lost in mystery again.',\n",
              " 'I have accepted a self which is wisdom engulfed by life.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DQi-fXYXIme",
        "outputId": "7eae8355-5d9a-48b7-f27f-2baec49b8cfe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply stopwords and filter then apply stemming.\n",
        "\n",
        "----> Apply stopwords\n",
        "\n",
        "----> see whether stopword is available for the word\n",
        "\n",
        "----> if not only then apply stemming"
      ],
      "metadata": {
        "id": "amKda_CgXOsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(document)):\n",
        "  words=nltk.word_tokenize(document[i])\n",
        "  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  document[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "zqbYvaLTXK3d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnfTn9UrYeFG",
        "outputId": "a934b929-5e61-4dc4-cc08-c01ef9faa38c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['`` i accept limit self .',\n",
              " 'how i affirm self i feel , , possibl ?',\n",
              " \"allendi may said : 'thi core , ' i never feel found substanc self , core .\",\n",
              " 'i feel space , illimit space .',\n",
              " 'the effect analysi wear way , .',\n",
              " 'the clear , stark vibrat life i soon .',\n",
              " 'i cover truth .',\n",
              " 'the veil sun .',\n",
              " 'eddi illus mysteri engulf realiti .',\n",
              " 'what i imagin true .',\n",
              " 'i want get lost mysteri .',\n",
              " 'i accept self wisdom engulf life .']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But this stemming does not perform very well. Which is why we'll use Snowball or Porter2."
      ],
      "metadata": {
        "id": "GklC3vjzYmDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "8GcMjAhrYy9N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snow_stem=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "rdEgqiGyYh3T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "yqs9xwDA9cWX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(document)):\n",
        "  words=nltk.word_tokenize(document[i])\n",
        "  words=[snow_stem.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  document[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "U9PiXf1qY8WF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc5KxLhXZGSH",
        "outputId": "023a96f8-1eb3-41a7-d5ca-d1495481c537"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['`` i accept limit self .',\n",
              " 'how i affirm self i feel , , possibl ?',\n",
              " \"allendi may said : this core , ' i never feel found substanc self , core .\",\n",
              " 'i feel space , illimit space .',\n",
              " 'the effect analysi wear way , .',\n",
              " 'the clear , stark vibrat life i soon .',\n",
              " 'i cover truth .',\n",
              " 'the veil sun .',\n",
              " 'eddi illus mysteri engulf realiti .',\n",
              " 'what i imagin true .',\n",
              " 'i want get lost mysteri .',\n",
              " 'i accept self wisdom engulf life .']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All letters become small case so no repitition of words take place."
      ],
      "metadata": {
        "id": "kuebbNoKZhKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Lemmatization"
      ],
      "metadata": {
        "id": "jh65FGJjEzPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "VWtqZg0uZHwG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgEnA2wzFnup",
        "outputId": "a754a5ee-3052-4172-a457-02dc7f396f9a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ezkmmnU5E4pj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "f2o_N4UzFQkl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(document)):\n",
        "  words=nltk.word_tokenize(document[i])\n",
        "  words=[lemma.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  document[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "de-2bmKHFAY7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BuDgywtFRi8",
        "outputId": "c83c1d75-078f-4cee-a011-4f3edee0a0ef"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['`` I accept limited self .',\n",
              " 'How I affirmable self I feel , , possibility ?',\n",
              " \"Allendy may said : 'This core , ' I never feel found substance self , core .\",\n",
              " 'I feel space , illimitable space .',\n",
              " 'The effect analysis wearing way , .',\n",
              " 'The clear , stark vibration life I soon .',\n",
              " 'I covered truth .',\n",
              " 'The veil sun .',\n",
              " 'Eddies illusion mystery engulfed reality .',\n",
              " 'What I imagine true .',\n",
              " 'I want get lost mystery .',\n",
              " 'I accepted self wisdom engulfed life .']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the results are better than stemming\n",
        "but it does not convert capital to lower the words"
      ],
      "metadata": {
        "id": "XWw_0gJwGORB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oh-Vm1IOFuHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}