{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization : Text Preprocessing"
      ],
      "metadata": {
        "id": "vUDk0t3viXFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the yransition performed on input text which may be paragraph (corpus) or sentences (document) to convert into tokens, i.e., paragraph to sentences, or paragraph to words or sentences to words."
      ],
      "metadata": {
        "id": "f4P3T0ljnCtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paragraph--------> Sentences"
      ],
      "metadata": {
        "id": "j0h8bR2ejhKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining corpus"
      ],
      "metadata": {
        "id": "hv6EHUDTifV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R6MUpkQNiTeP"
      },
      "outputs": [],
      "source": [
        "corpus=\"\"\"Dear Frau Milena. All writing seems futile to me, and it really is. the best would probably be for me to got to Vienna and take you away;I may even do it, although you don't want me to.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rKnyO77inaJ",
        "outputId": "d703acf5-c2ed-46e1-c9c2-1180338dfd2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Frau Milena. All writing seems futile to me, and it really is. the best would probably be for me to got to Vienna and take you away;I may even do it, although you don't want me to.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Library called tokenize and use functionality sent_tokenize that converts paragraph into sentences"
      ],
      "metadata": {
        "id": "wboXLcCxiyEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "UJbVVoPojBdY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "kycNM5Kbi19u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iGorDqGjFkH",
        "outputId": "1953cde7-310f-4e84-f767-13232eb915a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "xgtVeg0Si40o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf6OJPyCkqr_",
        "outputId": "58bdf676-8160-4c01-f015-6ef0fcba5c4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dear Frau Milena.',\n",
              " 'All writing seems futile to me, and it really is.',\n",
              " \"the best would probably be for me to got to Vienna and take you away;I may even do it, although you don't want me to.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Library called tokenize and use functionality word_tokenize that converts paragraph into words."
      ],
      "metadata": {
        "id": "JrgQb5vdjshL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paragraph-------> Words\n",
        "Sentences-------> Words"
      ],
      "metadata": {
        "id": "VIRuTRM0j77G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "jqxEPu9li8Bw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOS3PalvkDHn",
        "outputId": "eac84ed8-0354-4021-9d72-9fd747afc07c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dear',\n",
              " 'Frau',\n",
              " 'Milena',\n",
              " '.',\n",
              " 'All',\n",
              " 'writing',\n",
              " 'seems',\n",
              " 'futile',\n",
              " 'to',\n",
              " 'me',\n",
              " ',',\n",
              " 'and',\n",
              " 'it',\n",
              " 'really',\n",
              " 'is',\n",
              " '.',\n",
              " 'the',\n",
              " 'best',\n",
              " 'would',\n",
              " 'probably',\n",
              " 'be',\n",
              " 'for',\n",
              " 'me',\n",
              " 'to',\n",
              " 'got',\n",
              " 'to',\n",
              " 'Vienna',\n",
              " 'and',\n",
              " 'take',\n",
              " 'you',\n",
              " 'away',\n",
              " ';',\n",
              " 'I',\n",
              " 'may',\n",
              " 'even',\n",
              " 'do',\n",
              " 'it',\n",
              " ',',\n",
              " 'although',\n",
              " 'you',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'want',\n",
              " 'me',\n",
              " 'to',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentences in documents:\n",
        "  print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPJWRpafksZA",
        "outputId": "cd3fd4c2-bc40-4908-fed5-9f58f567ba9c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Frau Milena.\n",
            "All writing seems futile to me, and it really is.\n",
            "the best would probably be for me to got to Vienna and take you away;I may even do it, although you don't want me to.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentences in documents:\n",
        "  print(word_tokenize(sentences))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfvsuXCmkJBQ",
        "outputId": "3d6ab933-ea78-45ea-cc3d-a2e1daa65a1b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dear', 'Frau', 'Milena', '.']\n",
            "['All', 'writing', 'seems', 'futile', 'to', 'me', ',', 'and', 'it', 'really', 'is', '.']\n",
            "['the', 'best', 'would', 'probably', 'be', 'for', 'me', 'to', 'got', 'to', 'Vienna', 'and', 'take', 'you', 'away', ';', 'I', 'may', 'even', 'do', 'it', ',', 'although', 'you', 'do', \"n't\", 'want', 'me', 'to', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wordpunct_tokenize library"
      ],
      "metadata": {
        "id": "Oi4ertchlPvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see that even apostrophee is spit unlike in word_tokenize where n't is grouped together."
      ],
      "metadata": {
        "id": "BvMVKBZzlvvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "FjTNctmplfes"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtXz66wZk-oe",
        "outputId": "9490b955-8feb-4caf-c41b-aef826965e5e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dear',\n",
              " 'Frau',\n",
              " 'Milena',\n",
              " '.',\n",
              " 'All',\n",
              " 'writing',\n",
              " 'seems',\n",
              " 'futile',\n",
              " 'to',\n",
              " 'me',\n",
              " ',',\n",
              " 'and',\n",
              " 'it',\n",
              " 'really',\n",
              " 'is',\n",
              " '.',\n",
              " 'the',\n",
              " 'best',\n",
              " 'would',\n",
              " 'probably',\n",
              " 'be',\n",
              " 'for',\n",
              " 'me',\n",
              " 'to',\n",
              " 'got',\n",
              " 'to',\n",
              " 'Vienna',\n",
              " 'and',\n",
              " 'take',\n",
              " 'you',\n",
              " 'away',\n",
              " ';',\n",
              " 'I',\n",
              " 'may',\n",
              " 'even',\n",
              " 'do',\n",
              " 'it',\n",
              " ',',\n",
              " 'although',\n",
              " 'you',\n",
              " 'don',\n",
              " \"'\",\n",
              " 't',\n",
              " 'want',\n",
              " 'me',\n",
              " 'to',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TreebankWordTokenizer Library"
      ],
      "metadata": {
        "id": "9eVZYn4xmmiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fullstop is not treated as separate token. It is grouped with previous word."
      ],
      "metadata": {
        "id": "bWHF7h-Zmrzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "kpwX3O_MlbwN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####TreebankWordTokenizer has no arguemnets."
      ],
      "metadata": {
        "id": "reqKU2xJmSjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token=TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "waBXuA8kmJQC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrRhlti8mXMf",
        "outputId": "5b6b7805-01f0-4801-a0b3-e2c882084506"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dear',\n",
              " 'Frau',\n",
              " 'Milena.',\n",
              " 'All',\n",
              " 'writing',\n",
              " 'seems',\n",
              " 'futile',\n",
              " 'to',\n",
              " 'me',\n",
              " ',',\n",
              " 'and',\n",
              " 'it',\n",
              " 'really',\n",
              " 'is.',\n",
              " 'the',\n",
              " 'best',\n",
              " 'would',\n",
              " 'probably',\n",
              " 'be',\n",
              " 'for',\n",
              " 'me',\n",
              " 'to',\n",
              " 'got',\n",
              " 'to',\n",
              " 'Vienna',\n",
              " 'and',\n",
              " 'take',\n",
              " 'you',\n",
              " 'away',\n",
              " ';',\n",
              " 'I',\n",
              " 'may',\n",
              " 'even',\n",
              " 'do',\n",
              " 'it',\n",
              " ',',\n",
              " 'although',\n",
              " 'you',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'want',\n",
              " 'me',\n",
              " 'to',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jsfvSbzOl8Cg"
      }
    }
  ]
}